{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXYBWOJQEBLx"
      },
      "source": [
        "# Transformer Model Implementation\n",
        "\n",
        "Based on the paper \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "## 1. Big Picture\n",
        "- Transformer is a new model for translating text\n",
        "- It's different because it doesn't use traditional methods (no recurrence or convolution)\n",
        "- Uses \"attention\" to understand relationships between words\n",
        "\n",
        "## 2. Model Structure\n",
        "- Has an encoder (for input) and decoder (for output)\n",
        "- Both have 6 identical layers stacked on top of each other\n",
        "- Key innovation: Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1yxX7VzdEBL4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQjmjnjmEBL4"
      },
      "source": [
        "## 3. Positional Encoding\n",
        "- Problem: The model doesn't naturally understand word order\n",
        "- Solution: Add special codes to each word to indicate its position\n",
        "- Uses sine and cosine functions to create these codes\n",
        "- This allows the model to understand relative positions of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Iol39snVEBL4"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # Create an empty tensor to store positional encodings.\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Create a range of positions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # Scaling factor for positions.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even positions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd positions.\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)  # Add an extra dimension for batch and transpose dimensions.\n",
        "        self.register_buffer('pe', pe)  # Store positional encodings in a buffer so it's not trainable.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]  # Add positional encoding to input x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4oB2CbcEBL4"
      },
      "source": [
        "## 4. Transformer Model\n",
        "\n",
        "This model incorporates:\n",
        "- Multi-Head Attention: Allows the model to focus on different parts of the input simultaneously\n",
        "- Position-wise Feed-Forward Networks: Helps the model learn more complex patterns\n",
        "- Positional Encoding: Allows the model to understand word order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Kg2VVPnCEBL5"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5, seq_len=200):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)  # Initialize embedding weights.\n",
        "        self.decoder.bias.data.zero_()  # Initialize decoder biases to zero.\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)  # Initialize decoder weights.\n",
        "\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLavCSInEBL5"
      },
      "source": [
        "## 5. Model Setup and Training\n",
        "\n",
        "Here we set up the model with specific hyperparameters and demonstrate a basic training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2WPVQIcEBL5",
        "outputId": "cf55b968-f707-4a74-e717-287a5643ada0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 32, 10000])\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "ntokens = 10000  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0.2  # the dropout value\n",
        "\n",
        "# Create the model\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
        "\n",
        "# Example input\n",
        "src = torch.randint(0, ntokens, (10, 32))  # (sequence_length=10, batch_size=32)\n",
        "src_mask = torch.zeros((10, 10)).type(torch.bool)\n",
        "\n",
        "# Forward pass\n",
        "output = model(src, src_mask)\n",
        "print(output.shape)  # Should be (10, 32, ntokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRIvE_DHEBL5"
      },
      "source": [
        "## 6. Training Loop\n",
        "\n",
        "Here's a basic training loop. In practice, you'd need a proper dataset and data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPfSmHPbEBL5",
        "outputId": "44cec443-f5e8-4951-b662-9c49c2978a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.011198257096111774\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(10):  # Loop over epochs\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch in range(100):  # Assume 100 batches of training data\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        output = model(src, src_mask)  # Forward pass\n",
        "        loss = criterion(output.view(-1, ntokens), src.view(-1))  # Calculate loss\n",
        "        loss.backward()  # Backpropagate\n",
        "        optimizer.step()  # Update model weights\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a custom dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, ntokens, num_samples, seq_len):\n",
        "        super().__init__()\n",
        "        self.ntokens = ntokens\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly generate source and target sequences (dummy data)\n",
        "        src = torch.randint(0, self.ntokens, (self.seq_len,))\n",
        "        target = torch.randint(0, self.ntokens, (self.seq_len,))\n",
        "        return src, target\n",
        "\n",
        "# Parameters for the dataset\n",
        "ntokens = 10000  # Vocabulary size\n",
        "num_samples = 1000  # Number of samples in the test dataset\n",
        "seq_len = 10  # Length of each sequence\n",
        "\n",
        "# Create the test dataset and DataLoader\n",
        "test_dataset = TestDataset(ntokens, num_samples, seq_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "mTXlVXS8MYkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "avg_loss, accuracy = evaluate_model(model, test_loader, criterion, ntokens)\n"
      ],
      "metadata": {
        "id": "8DbCuo0-McBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assuming test_loader is your DataLoader for the test set\n",
        "avg_loss, accuracy = evaluate_model(model, test_loader, criterion, ntokens)\n"
      ],
      "metadata": {
        "id": "_aW_zvqtFe69"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKo-dzcrEBL5"
      },
      "source": [
        "## 7. Why It's Cool\n",
        "- Can process all words in parallel (very fast!)\n",
        "- Connects all words directly (helps with long-range dependencies)\n",
        "- Produces attention patterns that we can visualize and interpret\n",
        "\n",
        "## 8. Results\n",
        "- Achieves state-of-the-art results in translation tasks\n",
        "- Trains much faster than previous models"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}